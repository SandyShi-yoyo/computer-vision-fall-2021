{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "(Week 7) Stereo.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "RPenj5Yxj0mO"
      },
      "source": [
        "%matplotlib inline\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import cv2"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NCp4KNhvkakN"
      },
      "source": [
        "### Read this First\n",
        "\n",
        "#### Remember that `tab` is is useful for autocompletion.\n",
        "\n",
        "#### Remember that `shift + tab` is useful for rapidly obtaining usage + documentation."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VgQiOYkNKuYw"
      },
      "source": [
        "###**Rotations**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gb3F646Nu-96"
      },
      "source": [
        "**With understanding stereo, it is important to understand the concept of 3D rotations, since the relationship between the two cameras in a stereo setup often involves a rotation from one to the other.**\n",
        "\n",
        "**Let's explore how rotations can be represented in different ways. Run the custom_rotate below.**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Vdr0OiCpBaag"
      },
      "source": [
        "def custom_rotate(rotation_method, v1=None, v2=None, v3=None, v4=None, v5=None):\n",
        "  \"\"\" Plots five vectors in 3D. One vector is fixed, and the others are that vector\n",
        "      rotated by 90 degrees in the x axis, 90 degrees in the y axis, 90 degrees in\n",
        "      the z axis, and one with a more complex rotation.\n",
        "\n",
        "      The four plots are of the same graph, but with different viewpoints. Feel \n",
        "      free to edit the azimuth and/or elevation to any of the views if you want\n",
        "      to visualize your plot differently.\n",
        "\n",
        "      Args:\n",
        "        rotation_method: Function that performs the rotation\n",
        "\n",
        "  \"\"\"\n",
        "\n",
        "  # Create the vector and rotate them accordingly\n",
        "  vector = np.array([[11.231303753070549], [9.27144871768164], [18.085790226916288]]) if v1 is None else v1\n",
        "  vecto2 = rotation_method(vector, np.pi/2, 0, 0) if v2 is None else v2\n",
        "  vecto3 = rotation_method(vector, 0, np.pi/2, 0) if v3 is None else v3\n",
        "  vecto4 = rotation_method(vector, 0, 0, np.pi/2) if v4 is None else v4\n",
        "  vecto5 = rotation_method(vector, 0.5, 1.2, -0.2) if v5 is None else v5\n",
        "\n",
        "  # Helper to plot the vectors\n",
        "  def plot_transformed_vectors(ax):\n",
        "    ax.plot([vector[0], 0], [vector[1], 0], [vector[2], 0],color='k',zorder=1)\n",
        "    ax.plot([vecto2[0], 0], [vecto2[1], 0], [vecto2[2], 0],color='r',zorder=1)\n",
        "    ax.plot([vecto3[0], 0], [vecto3[1], 0], [vecto3[2], 0],color='g',zorder=1)\n",
        "    ax.plot([vecto4[0], 0], [vecto4[1], 0], [vecto4[2], 0],color='b',zorder=1)\n",
        "    ax.plot([vecto5[0], 0], [vecto5[1], 0], [vecto5[2], 0],color='m',zorder=1)\n",
        "    ax.set_xlabel('X direction')\n",
        "    ax.set_ylabel('Y direction')\n",
        "    ax.set_zlabel('Z direction')\n",
        "\n",
        "  # Create plot figure structure\n",
        "  fig = plt.figure(figsize=(12, 8))\n",
        "  ax1 = fig.add_subplot(221, projection='3d')\n",
        "  ax2 = fig.add_subplot(222, projection='3d')\n",
        "  ax3 = fig.add_subplot(223, projection='3d')\n",
        "  ax4 = fig.add_subplot(224, projection='3d')\n",
        "\n",
        "  # Plot the vector and its rotated versions\n",
        "  plot_transformed_vectors(ax1)\n",
        "  plot_transformed_vectors(ax2)\n",
        "  plot_transformed_vectors(ax3)\n",
        "  plot_transformed_vectors(ax4)\n",
        "\n",
        "  ax1.view_init(elev=10, azim=30)\n",
        "  ax2.view_init(elev=10, azim=90)\n",
        "  ax3.view_init(elev=10, azim=0)\n",
        "  ax4.view_init(elev=90, azim=0)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Prx8N_Q-vrEB"
      },
      "source": [
        "**Let's explore how to represent rotations as a matrix form, using fundamental rotations.**\n",
        "\n",
        "**Complete the function below, by writing in Rx, Ry, and Rz, which correspond to the fundamental rotations in x, y, and z. Run it to plot the rotations.**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "A1q28pXsHHel"
      },
      "source": [
        "def rotate_via_matrix_xyz(vector, phi=0, theta=0, xi=0):\n",
        "  \"\"\" Rotates vector by a rotation composed of x by angle phi, y by angle theta,\n",
        "      and z by angle xi. Does this by applying fundamental rotations RxRyRz.\n",
        "\n",
        "      Args:\n",
        "        vector: 3x1 numpy array of the vector\n",
        "        phi: rotation angle in radians around x axis\n",
        "        theta: rotation angle in radians around y axis\n",
        "        xi: rotation angle in radians around z axis\n",
        "\n",
        "      Return: \n",
        "        RxRyRz*vector: 3x1 numpy array of the rotated vector\n",
        "  \"\"\"\n",
        "\n",
        "  # Construct the fundamental rotations using np.cos and np.sin\n",
        "\n",
        "\n",
        "  return Rx @ Ry @ Rz @ vector\n",
        "\n",
        "custom_rotate(rotate_via_matrix_xyz)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qpa97c6_y5S7"
      },
      "source": [
        "**What coloured lines correspond to what rotations?**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r8OXDKr_y76Z"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zjuTJC49y8WP"
      },
      "source": [
        "**One important property with using rotation matrix representations is that they are not commutative. That is, the order of rotations matter!.**\n",
        "\n",
        "**Complete the function below, again just filling in the representations for the fundamental rotations below (they are identical to above, I'm just not giving them to you so you won't copy it in the first part). Notice that now, the rotation RzRyRx instead of RxRyRz.**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zWUJG9_aKYOz"
      },
      "source": [
        "def rotate_via_matrix_zyx(vector, phi=0, theta=0, xi=0):\n",
        "  \"\"\" Rotates vector by a rotation composed of x by angle phi, y by angle theta,\n",
        "      and z by angle xi. Does this by applying fundamental rotations RxRyRz.\n",
        "\n",
        "      Args:\n",
        "        vector: 3x1 numpy array of the vector\n",
        "        phi: rotation angle in radians around x axis\n",
        "        theta: rotation angle in radians around y axis\n",
        "        xi: rotation angle in radians around z axis\n",
        "\n",
        "      Return: \n",
        "        RzRyRx*vector: 3x1 numpy array of the rotated vector\n",
        "  \"\"\"\n",
        "\n",
        "  # Construct the fundamental rotations using np.cos and np.sin\n",
        "\n",
        "\n",
        "  return Rz @ Ry @ Rx @ vector\n",
        "\n",
        "custom_rotate(rotate_via_matrix_zyx)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Zhv2t1Wc0ImS"
      },
      "source": [
        "**Instead of rotation matrices, we can also represent rotations using quaternions, which you can think of for now as a complex-number type representation of a rotation that contains a scalar part and a vector part. See below four different quaternions.**\n",
        "\n",
        "**Below, apply these rotations to vector v1, and plot them using custom_rotate.**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cYpvXf4ADtr0"
      },
      "source": [
        "from scipy.spatial.transform import Rotation as R\n",
        "\n",
        "# Original vector\n",
        "v1 = np.array([11.231303753070549, 9.27144871768164, 18.085790226916288])\n",
        "\n",
        "# Four different quaternion rotations\n",
        "r2 = R.from_quat([ 0.7071068, 0, 0, 0.7071068 ])\n",
        "r3 = R.from_quat([0, 0.7071068, 0, 0.7071068])\n",
        "r4 = R.from_quat([ 0, 0, 0.7071068, 0.7071068 ])\n",
        "r5 = R.from_quat([ -0.0719725, 0.6031058, -0.1374307, 0.7824296 ])\n",
        "\n",
        "# Apply the rotations to our vector via rx.apply(v1)\n",
        "\n",
        "\n",
        "# Plots\n",
        "custom_rotate(None, v1, v2, v3, v4, v5)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kW7PAO3T2IPE"
      },
      "source": [
        "**What kind of rotations (i.e. what angle in x, y, z axis rotations) do each of the quaternions represent?**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j31a-qQy2NsG"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QTjLjpOo1m5m"
      },
      "source": [
        "**We can also represent rotations using a rotation angle representation. Rotations in 3D can always be represented by a rotation around a vector/axis by an angle, and this rotation vector representation is exactly this, with the direction representing the axis of rotation, and the magnitude of the vector representing the amount of rotation.**\n",
        "\n",
        "**Like with the quaternions above, apply these rotations to vector v1, and plot them using custom_rotate.**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jv7mVYRCJ0Yl"
      },
      "source": [
        "# Four different rotations using rotation vector representation\n",
        "r1 = R.from_rotvec([np.pi/2, 0, 0])\n",
        "r2 = R.from_rotvec([0, np.pi/2, 0])\n",
        "r3 = R.from_rotvec([0, 0, np.pi/2])\n",
        "r4 = R.from_rotvec([ -0.1553866, 1.3020895, -0.2967093 ])\n",
        "\n",
        "# Apply the rotations to our vector via rx.apply(v1)\n",
        "\n",
        "\n",
        "# Plots\n",
        "custom_rotate(None, v1, v2, v3, v4, v5)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pSXIY3Md3CFL"
      },
      "source": [
        "**What kind of rotations (i.e. what angle in x, y, z axis rotations) do each of the representations**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "29UUu4_23HwS"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XaeCYC4wKyJi"
      },
      "source": [
        "###**Disparity maps**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dvYlWmvn4Q2u"
      },
      "source": [
        "**Now let's explore stereo! Stereo involves two camera images of the same scene, at slightly different angles, and we are able to determine the depth of an object by comparing the locations of that object in those images.**\n",
        "\n",
        "**We can explore how to do this by forming a disparity map, which shows, for every pixel, the depth of that object in the scene.**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y_dJPND-4vro"
      },
      "source": [
        "**Run the code below to import the image and display them horizontally.**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Dz1vEoE89Nbn"
      },
      "source": [
        "imgL = cv2.imread('tsukuba_l.png',0)\n",
        "imgR = cv2.imread('tsukuba_r.png',0)\n",
        "\n",
        "plt.figure(figsize=(12, 8))\n",
        "plt.imshow(cv2.hconcat([imgL, imgR]), cmap='gray')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IfvwckG-43Mt"
      },
      "source": [
        "**We can use OpenCV's cv2.StereoBM_create to create the disparity map. Set the numDisparities=16 and blockSize=15, and then use the .compute function to create the disaprity map. Display the resulting disparity map.**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PikQKNLH9NTt"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gAj2nREb5R0R"
      },
      "source": [
        "**We see that the disparity map is pretty impressive and close to what we would expect. Now let's implement our own version to see how well we can do compared to this.**\n",
        "\n",
        "**Because the relationships between our left and right cameras is a pure horizontal translation, we know that every pixel on the left image can only appear in the same row in the right image. Hence for every pixel in the left image, we have to find the same pixel in the right image on the same row.**\n",
        "\n",
        "**To find the best match, we can take a window around the pixel in question in the left image, and slide a window across the right image and comparing them at each position. The position at which the windows have the best match is our disparity for that pixel.**\n",
        "\n",
        "**Hence we need a metric determine how well two patches/windows correspond to each other. Let's implement a few.**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uI-9i0hS6NQs"
      },
      "source": [
        "**Implement SAD matching below between the left_roi and right_roi. If the two rois are of different sizes, return a default -1 value.**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HV_MGn99PT0l"
      },
      "source": [
        "def SAD(left_roi, right_roi):\n",
        "  \"\"\" Sum of Absolute Differences metric to compare two roi patches\n",
        "\n",
        "      Args:\n",
        "        left_roi: 2D numpy array containing the left image patch \n",
        "        right_roi: 2D numpy array containing the right image patch\n",
        "\n",
        "      Return:\n",
        "        score: The score of the match\n",
        "\n",
        "  \"\"\"\n",
        "\n",
        "  # Implement this here\n",
        "  \n",
        "\n",
        "  return score\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WKMMqyBD6_B2"
      },
      "source": [
        "**Implement SSD matching below between the left_roi and right_roi. If the two rois are of different sizes, return a default -1 value.**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jKpdGU92PlFe"
      },
      "source": [
        "def SSD(left_roi, right_roi):\n",
        "  \"\"\" Sum of Squared Differences metric to compare two roi patches\n",
        "\n",
        "      Args:\n",
        "        left_roi: 2D numpy array containing the left image patch \n",
        "        right_roi: 2D numpy array containing the right image patch\n",
        "\n",
        "      Return:\n",
        "        score: The score of the match\n",
        "\n",
        "  \"\"\"\n",
        "\n",
        "  # Implement this here\n",
        "  \n",
        "\n",
        "  return score"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Rn6L82FX7Akd"
      },
      "source": [
        "**Implement NCC matching below between the left_roi and right_roi. If the two rois are of different sizes, return a default -1 value.**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3KixhLiMPztd"
      },
      "source": [
        "def NCC(left_roi, right_roi):\n",
        "  \"\"\" Normalized Cross Correlation metric to compare two roi patches\n",
        "\n",
        "      Args:\n",
        "        left_roi: 2D numpy array containing the left image patch \n",
        "        right_roi: 2D numpy array containing the right image patch\n",
        "\n",
        "      Return:\n",
        "        score: The score of the match\n",
        "\n",
        "  \"\"\"\n",
        "\n",
        "  # Implement this here\n",
        "  \n",
        "\n",
        "  return score"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5MB3vfaq6aGb"
      },
      "source": [
        "**If two patches are a perfect match, would you expect the return values for these three to be large or small? Why can we return -1 if the sizes do not match and have confidence that no two rois will lead to a score of -1?**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MPGwtdUq6lr7"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_5GbSvQj6nkf"
      },
      "source": [
        "**Now that we have a way of comparing how similar two patches are, let's write a function that compares a patch from the left image to all of the patches in the right image on the same row. For implementation simplicity, the \"window\" that we take around our pixel is from [y:y+window_size, x:x+window_size]. Fill in the blanks below.**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "izVlMpUZspVY"
      },
      "source": [
        "def compare_blocks(y, x, left_patch, right_image, window_size, max_disparity, criterion):\n",
        "  \"\"\" Finds the best match between the left_patch and a patch from the right_image. Because\n",
        "      we have a non-verged system, we only have to check along the same row as our patch.\n",
        "\n",
        "      Args:\n",
        "        y: The y coordinate of the top left pixel in the left_patch\n",
        "        x: The x coordinate of the top left pixel in the left_patch\n",
        "        left_patch: The template patch from left image to match\n",
        "        right_image: The right image that we're trying to search for the best match for\n",
        "        window_size: The length of one side of the square patch\n",
        "        max_disparity: The maximum disparity that we need to check through\n",
        "        criterion: How we compare similarity between two patches\n",
        "      \n",
        "      Return:\n",
        "        min_index: (y, x) the coordinate in the right image that is the best match\n",
        "\n",
        "  \"\"\"\n",
        "\n",
        "  # Get search range for the right image (we only have to search a maximum of max_disparity patches)\n",
        "  x_min = max(0, x - max_disparity)\n",
        "  x_max = min(right_image.shape[1], x + max_disparity)\n",
        "\n",
        "  # Minimum SAD score, and the patch location (y, x) that corresponds to this minimum score\n",
        "  min_sad = None\n",
        "  min_index = None\n",
        "\n",
        "  for x in range(x_min, x_max):\n",
        "\n",
        "      # Get the patch from the right image at (y, x)\n",
        "      right_patch = \n",
        "\n",
        "      # Get the score\n",
        "      sad = criterion(left_patch, right_patch)\n",
        "\n",
        "      # Update min_sad and min_index appropriately\n",
        "\n",
        "\n",
        "  return min_index"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rHXuCt9oE07s"
      },
      "source": [
        "**Now we just have to loop through every patch in the left image to find the corresponding patch in the right image.**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-wIpJxT-s0o3"
      },
      "source": [
        "def get_disparity_map(imgL, imgR, window_size, max_disparity, criterion):\n",
        "  \"\"\" Creates the disparity map by getting the x-displacement that corresponds to the best \n",
        "      match in the right image for every pixel in the left image.\n",
        "\n",
        "      Args:\n",
        "        imgL: The left image\n",
        "        imgR: The right image\n",
        "        window_size: The dimension of the patch to compare around the pixel\n",
        "        max_disparity: \n",
        "\n",
        "  \"\"\"\n",
        "  imgL = imgL.astype(int)\n",
        "  imgR = imgR.astype(int)\n",
        "  h, w = imgL.shape\n",
        "  disparity_map = np.zeros((h, w))\n",
        "\n",
        "  # Go over each pixel position\n",
        "  for y in range(0, h-window_size):\n",
        "      for x in range(0, w-window_size):\n",
        "\n",
        "          # Get the window for pixel (y, x) from imgL\n",
        "          # Remember that our window is not centered around the pixel but instead has top left corner at (y, x)\n",
        "\n",
        "\n",
        "          # Find the minimum index, and fill in the disparity map accordingly\n",
        "          min_index = compare_blocks(y, x, left_patch, imgR, window_size, max_disparity, criterion)\n",
        "          disparity_map[y, x] = abs(min_index[1] - x)\n",
        "  \n",
        "  return disparity_map"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Rn16B2UgMbDx"
      },
      "source": [
        "**Let's give it a try! Run get_disparity_map with imgL and imgR, with window_size=7, max_disparity=56, and criterion=SAD. This might take a while (ãround 90 seconds for me on Google Colab). Display the result, along with a colorbar.**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QVHMW7EAtBzB"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BJ3Y4MAlNMzo"
      },
      "source": [
        "**Do objects closer to the camera have a higher or lower disparity? Why is that so?**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zY9rMp3nNZVG"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rwNsBWKtNZwn"
      },
      "source": [
        "**Notice that we defined our window at (y, x) to have the top left corner at (y, x) instead of centered around (y, x), yet we still get a good result - why is that?**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TSZYa7LLNhC_"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hl6Aaf51Nh3E"
      },
      "source": [
        "**Now run it on a larger window_size of 21, and display the result.**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GKTL8XXGz6vZ"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OUvYUjUvNptp"
      },
      "source": [
        "**Now run it with window_size=7 and the criterion now SSD instead of SAD. Display the result.**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kjhj-Wxd3-15"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JXpWuTpHNwt0"
      },
      "source": [
        "**Do the same above, this time using NCC.**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tWnIvS1a4BY9"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jJitX3P3N5Ou"
      },
      "source": [
        "**After running those, you're probably annoyed because they take a while to run. So let's speed it up. Observe that finding the disparity for every single row is indepedent of each other, so we can find the disparities for each row simultaneously. We simply need to shift the entire right image one pixel at a time, and each time, we compute the pixel-wise difference between the left image and the shifted right image. Hence we now have $diffs(i, j) = left(i, j) - right(i, j + k)$, where $k$ is the amount of shift.** \n",
        "\n",
        "**Since we are interested in the sum of these differences in a window around each pixel (since we are implementing SAD), we can simply convolve the difference matrix with a uniform box filter to get the score for each pixel!**\n",
        "\n",
        "**Fill in the blanks below, and run it, observing how much faster it is.**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eW4iJSpG7Oo_"
      },
      "source": [
        "rows, cols = imgL.shape   # Image dimensions\n",
        "num_disparities = 56      # number of disparities to check\n",
        "block = 7                 # window size to match\n",
        "\n",
        "# Create the block by block sized kernel, normalized to have unit sum\n",
        "\n",
        "\n",
        "# Create a disparity map that saves the score for each disparity \n",
        "disparity_maps = np.zeros([imgL.shape[0], imgL.shape[1], num_disparities])\n",
        "\n",
        "# We loop through for each disparity we have to shift by\n",
        "for d in range(0, num_disparities):\n",
        "\n",
        "    # Shift imgR by d pixels to the right\n",
        "\n",
        "\n",
        "    # Calculate the absolute differences between each pixel in imgL and the shifted imgR\n",
        "\n",
        "\n",
        "    # Convolve the resulting difference matrix with the kernel\n",
        "    filtered_image = \n",
        "\n",
        "    # Filtered image has the SAD of each pixel for this disparity\n",
        "    disparity_maps[:, :, d] = filtered_image\n",
        "\n",
        "# We then find the minimum disparity value at each pixel and normalize the scale\n",
        "disparity = np.argmin(disparity_maps, axis=2)\n",
        "disparity = np.uint8(disparity * 255 / num_disparities)\n",
        "\n",
        "# Display the image\n",
        "plt.imshow(disparity, cmap='hot')\n",
        "plt.colorbar()\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zmNOuYLi3oyV"
      },
      "source": [
        "###**Camera Calibration with OpenCV**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uXcLxpFURFvx"
      },
      "source": [
        "**Here's just a quick run-throug with the camera calibration from OpenCV - this is mainly to get you to make sure that you know what the different components to calibrate a camera are when using OpenCV. Fill in the blanks, and answer the questions below.**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UBkHJNO5SYk6"
      },
      "source": [
        "# termination criteria\n",
        "criteria = (cv2.TERM_CRITERIA_EPS + cv2.TERM_CRITERIA_MAX_ITER, 30, 0.001)\n",
        "\n",
        "# prepare object points, like (0,0,0), (1,0,0), (2,0,0) ....,(6,5,0)\n",
        "objp = np.zeros((6*7,3), np.float32)\n",
        "objp[:,:2] = np.mgrid[0:7,0:6].T.reshape(-1,2)\n",
        "\n",
        "# Arrays to store object points and image points from all the images.\n",
        "objpoints = [] # 3d point in real world space\n",
        "imgpoints = [] # 2d points in image plane.\n",
        "\n",
        "images = ['left01.jpg', 'left02.jpg', 'left03.jpg', 'left04.jpg', 'left05.jpg']\n",
        "for fname in images:\n",
        "    img = cv2.imread(fname)\n",
        "    gray = cv2.cvtColor(img,cv2.COLOR_BGR2GRAY)\n",
        "\n",
        "    # Find the chess board corners\n",
        "    ret, corners = \n",
        "\n",
        "    # If found, add object points, image points (after refining them)\n",
        "    if ret == True:\n",
        "        objpoints.append(objp)\n",
        "\n",
        "        # Compute subpixel corner accuracy via cv2.cornerSubPix\n",
        "        corners2 = \n",
        "        imgpoints.append(corners2)\n",
        "\n",
        "        # Draw and display the corners\n",
        "        img = cv2.drawChessboardCorners(img, (7,6), corners2,ret)\n",
        "        plt.figure()\n",
        "        plt.imshow(img)\n",
        "\n",
        "ret, mtx, dist, rvecs, tvecs = cv2.calibrateCamera(objpoints, imgpoints, gray.shape[::-1],None,None)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6bANYW7aRTM2"
      },
      "source": [
        "**When calling cv2.findChessboardCorners, one of the arguments is (7, 6). What does this represent?**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E7kW97fERa0e"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Scn_JBDhRjQi"
      },
      "source": [
        "**What does each argument in cv2.cornerSubPix represent?**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "flMxSPqJRl_k"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rqF4uoCZRqhY"
      },
      "source": [
        "**Feel free to pick any of the images and perform unwarping of them below. Display the results.**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hoUlxtb9WsOv"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}